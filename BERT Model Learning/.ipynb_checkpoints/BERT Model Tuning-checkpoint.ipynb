{"cells":[{"cell_type":"code","source":["#%md\n\n# Mount Azure storage\n### To mount Azure storage, use the following code and run with your desired container name and mount point. Or in this notebook, just comment out the \"%md\" and run this cell.\n\n\ndbutils.fs.unmount(\"/mnt/test-projects\")\n\ndbutils.fs.mount(\n  source = \"wasbs://test-projects@ardsteamstorage1.blob.core.windows.net\",\n  mount_point = \"/mnt/test-projects\",\n  extra_configs = {\"fs.azure.account.key.ardsteamstorage1.blob.core.windows.net\":\"2Gacmofk/DpJgWzJ8rou/HQ0Ws6AD+CqWdQLB4Qbz3n5CIYW22d378cObhtIfPHKaYCQcc1bLGPOn/dDAPLNWg==\"})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eee5cab0-db55-4549-a093-43e34ba991c9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/test-projects has been unmounted.\nOut[1]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/test-projects has been unmounted.\nOut[1]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a117c5f1-2bf2-4d58-a0a3-b6ece12e982a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">There are 2 GPU(s) available.\nWe will use the GPU: Tesla K80\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">There are 2 GPU(s) available.\nWe will use the GPU: Tesla K80\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import tensorflow as tf\n\ntf.test.gpu_device_name()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad3efcbc-9270-4194-91ae-78eb9e01cfe4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: &#39;/device:GPU:0&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &#39;/device:GPU:0&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Installing huggingface library\n\nWe installed the transformer library in the Library tab inside the databricks clusters settings."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"72d87d5b-0b7a-470f-82a5-68713422cc48"}}},{"cell_type":"markdown","source":["# Loading CoLA Dataset\n\nWe will use \"The Corpus of Linguistic Acceptibility (CopLA)\" dataset for single sentence classification. It is a set of sentences labeled as grammatically correct or in correct. This is included in the **GLUE Benchmark** on which models like BERT are cometing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8172d2d-0dc8-409b-af51-d5fd730438fa"}}},{"cell_type":"markdown","source":["## Download and Extract Data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8e26ca6-afdd-4f68-96f3-2f6e49d2deed"}}},{"cell_type":"code","source":["%sh\n\nwget https://nyu-mll.github.io/CoLA/cola_public_1.1.zip"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93b22832-1889-47b7-9e00-9d30f005e24d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">--2021-06-10 20:20:37--  https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\nResolving nyu-mll.github.io (nyu-mll.github.io)... 185.199.110.153, 185.199.108.153, 185.199.109.153, ...\nConnecting to nyu-mll.github.io (nyu-mll.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 255330 (249K) [application/zip]\nSaving to: ‘cola_public_1.1.zip’\n\n     0K .......... .......... .......... .......... .......... 20% 9.96M 0s\n    50K .......... .......... .......... .......... .......... 40% 10.3M 0s\n   100K .......... .......... .......... .......... .......... 60% 21.7M 0s\n   150K .......... .......... .......... .......... .......... 80% 58.4M 0s\n   200K .......... .......... .......... .......... ......... 100% 16.2M=0.02s\n\n2021-06-10 20:20:37 (15.5 MB/s) - ‘cola_public_1.1.zip’ saved [255330/255330]\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2021-06-10 20:20:37--  https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\nResolving nyu-mll.github.io (nyu-mll.github.io)... 185.199.110.153, 185.199.108.153, 185.199.109.153, ...\nConnecting to nyu-mll.github.io (nyu-mll.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 255330 (249K) [application/zip]\nSaving to: ‘cola_public_1.1.zip’\n\n     0K .......... .......... .......... .......... .......... 20% 9.96M 0s\n    50K .......... .......... .......... .......... .......... 40% 10.3M 0s\n   100K .......... .......... .......... .......... .......... 60% 21.7M 0s\n   150K .......... .......... .......... .......... .......... 80% 58.4M 0s\n   200K .......... .......... .......... .......... ......... 100% 16.2M=0.02s\n\n2021-06-10 20:20:37 (15.5 MB/s) - ‘cola_public_1.1.zip’ saved [255330/255330]\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["ls"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b265286-3717-4203-8a18-a895b2e4a2ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">cola_public_1.1.zip  conf/  eventlogs/  ganglia/  logs/\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">cola_public_1.1.zip  conf/  eventlogs/  ganglia/  logs/\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\nunzip cola_public_1.1.zip"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa8e9c44-6cba-4a3d-af97-452b6435e937"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Archive:  cola_public_1.1.zip\n   creating: cola_public/\n  inflating: cola_public/README      \n   creating: cola_public/tokenized/\n  inflating: cola_public/tokenized/in_domain_dev.tsv  \n  inflating: cola_public/tokenized/in_domain_train.tsv  \n  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n   creating: cola_public/raw/\n  inflating: cola_public/raw/in_domain_dev.tsv  \n  inflating: cola_public/raw/in_domain_train.tsv  \n  inflating: cola_public/raw/out_of_domain_dev.tsv  \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Archive:  cola_public_1.1.zip\n   creating: cola_public/\n  inflating: cola_public/README      \n   creating: cola_public/tokenized/\n  inflating: cola_public/tokenized/in_domain_dev.tsv  \n  inflating: cola_public/tokenized/in_domain_train.tsv  \n  inflating: cola_public/tokenized/out_of_domain_dev.tsv  \n   creating: cola_public/raw/\n  inflating: cola_public/raw/in_domain_dev.tsv  \n  inflating: cola_public/raw/in_domain_train.tsv  \n  inflating: cola_public/raw/out_of_domain_dev.tsv  \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["ls cola_public/"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2786b190-cb49-4ea6-85dd-f6aee859fa2a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">README  raw/  tokenized/\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">README  raw/  tokenized/\r\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Parse\n\nWe can see from the file names that both `tokenized` and `raw` versions of the data are available. \n\nWe can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we *must* use the tokenizer provided by the model. This is because \n1. the model has a specific, fixed vocabulary and \n2. the BERT tokenizer has a particular way of handling out-of-vocabulary words."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2626d24-4e98-48db-b98d-ee416c3ab969"}}},{"cell_type":"code","source":["import pandas as pd\n\ndf = pd.read_csv('cola_public/raw/in_domain_train.tsv', delimiter='\\t', header=None, names=['source', 'label', 'label_notes', 'sentence'])\n\ndf.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f0b4d39-7fca-4e69-96b3-c13b16ff2527"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[8]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Our friends won't buy this analysis, let alone...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization and I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization or I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>The more we study verbs, the crazier they get.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Day by day the facts are getting murkier.</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Our friends won't buy this analysis, let alone...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization and I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization or I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>The more we study verbs, the crazier they get.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Day by day the facts are getting murkier.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.sample(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"130794a9-5cd2-46cd-a372-7581561698d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7808</th>\n      <td>ad03</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Why did you eat the cake?</td>\n    </tr>\n    <tr>\n      <th>3705</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>He talked to a girl about swimming.</td>\n    </tr>\n    <tr>\n      <th>1148</th>\n      <td>r-67</td>\n      <td>0</td>\n      <td>*</td>\n      <td>I sent him it.</td>\n    </tr>\n    <tr>\n      <th>3814</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>John studied hard to pass the exam.</td>\n    </tr>\n    <tr>\n      <th>1555</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Tom ordered bacon, and Dick ordered lettuce, a...</td>\n    </tr>\n    <tr>\n      <th>2434</th>\n      <td>l-93</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Columbus believed that the earth was round.</td>\n    </tr>\n    <tr>\n      <th>1275</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>I went to the store and Nike bought some whisky.</td>\n    </tr>\n    <tr>\n      <th>3939</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>The teacher became tired of the students.</td>\n    </tr>\n    <tr>\n      <th>7464</th>\n      <td>sks13</td>\n      <td>0</td>\n      <td>*</td>\n      <td>Will not John go to school?</td>\n    </tr>\n    <tr>\n      <th>1938</th>\n      <td>r-67</td>\n      <td>0</td>\n      <td>*</td>\n      <td>I can't remember the name of somebody who had ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7808</th>\n      <td>ad03</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Why did you eat the cake?</td>\n    </tr>\n    <tr>\n      <th>3705</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>He talked to a girl about swimming.</td>\n    </tr>\n    <tr>\n      <th>1148</th>\n      <td>r-67</td>\n      <td>0</td>\n      <td>*</td>\n      <td>I sent him it.</td>\n    </tr>\n    <tr>\n      <th>3814</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>John studied hard to pass the exam.</td>\n    </tr>\n    <tr>\n      <th>1555</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Tom ordered bacon, and Dick ordered lettuce, a...</td>\n    </tr>\n    <tr>\n      <th>2434</th>\n      <td>l-93</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Columbus believed that the earth was round.</td>\n    </tr>\n    <tr>\n      <th>1275</th>\n      <td>r-67</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>I went to the store and Nike bought some whisky.</td>\n    </tr>\n    <tr>\n      <th>3939</th>\n      <td>ks08</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>The teacher became tired of the students.</td>\n    </tr>\n    <tr>\n      <th>7464</th>\n      <td>sks13</td>\n      <td>0</td>\n      <td>*</td>\n      <td>Will not John go to school?</td>\n    </tr>\n    <tr>\n      <th>1938</th>\n      <td>r-67</td>\n      <td>0</td>\n      <td>*</td>\n      <td>I can't remember the name of somebody who had ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sentences = df.sentence\nlabels = df.label"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"746081aa-2068-4666-a256-0497ccc0cf3a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Tokenization and formatting"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07388eee-c41b-4fc9-a5ab-2c547e90a8cf"}}},{"cell_type":"markdown","source":["## BERT Tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"631622a4-775d-41f6-aaa9-a91491343c09"}}},{"cell_type":"code","source":["from transformers import BertTokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fce9c41-25b6-496d-a4aa-a3d042032e10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"570c6871-fa97-40c2-bdb7-87e70ffc8eb2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rDownloading:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 232k/232k [00:00&lt;00:00, 15.2MB/s]\n\rDownloading:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 28.0/28.0 [00:00&lt;00:00, 50.9kB/s]\n\rDownloading:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 466k/466k [00:00&lt;00:00, 18.7MB/s]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rDownloading:   0%|          | 0.00/232k [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 232k/232k [00:00&lt;00:00, 15.2MB/s]\n\rDownloading:   0%|          | 0.00/28.0 [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 28.0/28.0 [00:00&lt;00:00, 50.9kB/s]\n\rDownloading:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 466k/466k [00:00&lt;00:00, 18.7MB/s]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print('Original sentence: ', sentences[0])\n\nprint('Tokenized sentence: ', tokenizer.tokenize(sentences[0]))\n\nprint(\"Token ID's :\", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c6396f5-72e5-49f9-91b5-3874a8e19ee5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original sentence:  Our friends won&#39;t buy this analysis, let alone the next one we propose.\nTokenized sentence:  [&#39;our&#39;, &#39;friends&#39;, &#39;won&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;buy&#39;, &#39;this&#39;, &#39;analysis&#39;, &#39;,&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;the&#39;, &#39;next&#39;, &#39;one&#39;, &#39;we&#39;, &#39;propose&#39;, &#39;.&#39;]\nToken ID&#39;s : [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original sentence:  Our friends won&#39;t buy this analysis, let alone the next one we propose.\nTokenized sentence:  [&#39;our&#39;, &#39;friends&#39;, &#39;won&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;buy&#39;, &#39;this&#39;, &#39;analysis&#39;, &#39;,&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;the&#39;, &#39;next&#39;, &#39;one&#39;, &#39;we&#39;, &#39;propose&#39;, &#39;.&#39;]\nToken ID&#39;s : [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Formatting\nBERT requires some required formatting. Some of them sounds redundent and some can be easily infered from the data. But that's how they decided to build it.\nWe are required to:\n1. Add special tokens to the start and end of each sentence.\n2. Pad & truncate all sentences to a single constant length.\n3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bd5f228-c5a2-49e2-a47a-cb488f6073da"}}},{"cell_type":"markdown","source":["### Required tokens\n\n\n**`[SEP]`**\n\nAt the end of every sentence, we need to append the special `[SEP]` token. \n\nThis token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?). \n\nI am not certain yet why the token is still required when we have only single-sentence input, but it is!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7bfeeb2e-c5e3-4047-90b9-29a5383ce65d"}}},{"cell_type":"markdown","source":["**`[CLS]`**\n\nFor classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence.\n\nThis token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).\n\n![Illustration of CLS token purpose](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)\n\nOn the output of the final (12th) transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier*.\n\n>  \"The first token of every sequence is always a special classification token (`[CLS]`). The final hidden state\ncorresponding to this token is used as the aggregate sequence representation for classification\ntasks.\" (from the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n\nYou might think to try some pooling strategy over the final embeddings, but this isn't necessary. Because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector. It's already done the pooling for us!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d80d801-3835-49de-8fa8-faff14fe9cd2"}}},{"cell_type":"markdown","source":["### Sentence Length and attention mask\n\nThe sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n\nBERT has two constraints:\n1. All sentences must be padded or truncated to a single, fixed length.\n2. The maximum sentence length is 512 tokens.\n\nPadding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n\n<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n\nThe \"Attention Mask\" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?!). This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n\nThe maximum length does impact training and evaluation speed, however. \nFor example, with a Tesla K80:\n\n`MAX_LEN = 128  -->  Training epochs take ~5:28 each`\n\n`MAX_LEN = 64   -->  Training epochs take ~2:57 each`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0809b649-b1d2-4abb-b5bb-d9938609c483"}}},{"cell_type":"markdown","source":["## Tokenize Dataset\n\nThe transformers library provides a helpful `encode` function which will handle most of the parsing and data prep steps for us.\n\nBefore we are ready to encode our text, though, we need to decide on a **maximum sentence length** for padding / truncating to.\n\nThe below cell will perform one tokenization pass of the dataset in order to measure the maximum sentence length.\n\n#### Test Tokenize"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db3df97b-72cd-43b7-8347-35bb5816138f"}}},{"cell_type":"code","source":["max_len = 0\n\nfor sent in sentences:\n  \n  input_ids = tokenizer.encode(sent, add_special_tokens=True)\n  \n  max_len = max(max_len, len(input_ids))\n  \nprint(\"Maximum sentence length is %d\"%max_len)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a3a1efb-35ed-4fdf-bfbd-d9f9d5e567b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Maximum sentence length is 47\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Maximum sentence length is 47\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["In case there are some longer test sentences, we will set the max length to 64"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"555f1214-8d53-475e-ae76-c1bac2b29209"}}},{"cell_type":"markdown","source":["### Real Tokenize\n\nNow we're ready to perform the real tokenization.\n\nThe `tokenizer.encode_plus` function combines multiple steps for us:\n\n1. Split the sentence into tokens.\n2. Add the special `[CLS]` and `[SEP]` tokens.\n3. Map the tokens to their IDs.\n4. Pad or truncate all sentences to the same length.\n5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n\nThe first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fa47358-7c9d-40d0-913e-ecd14bc07091"}}},{"cell_type":"code","source":["input_ids = []\nattention_masks = []\n\nfor sent in sentences:\n  \n  encoded_dict = tokenizer.encode_plus(sent, \n                                       add_special_tokens=True,\n                                      max_length=64, \n                                       pad_to_max_length = True,\n                                      return_attention_mask=True, \n                                       return_tensors='pt')\n  \n  \n  input_ids.append(encoded_dict['input_ids'])\n  \n  attention_masks.append(encoded_dict['attention_mask'])\n  \n  \ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim = 0)\nlabels = torch.tensor(labels)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bda9517-2055-4372-b8f5-a8b61dc1a485"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/databricks/python/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to &#39;longest_first&#39; truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/databricks/python/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2104: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print('Original: ', sentences[0])\nprint('Token ID: ', input_ids[0])\nprint('Attention : ', attention_masks[0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c7ebe0fb-6415-44f6-b81a-834f527699b7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Original:  Our friends won&#39;t buy this analysis, let alone the next one we propose.\nToken ID:  tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\nAttention :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Original:  Our friends won&#39;t buy this analysis, let alone the next one we propose.\nToken ID:  tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0])\nAttention :  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(sentences.size)\nprint(input_ids.size())\nprint(attention_masks.size())\nsentences[0][0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e5e806bf-64af-4c15-a0d8-59b57f6b9353"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">8551\ntorch.Size([8551, 64])\ntorch.Size([8551, 64])\nOut[34]: &#39;O&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">8551\ntorch.Size([8551, 64])\ntorch.Size([8551, 64])\nOut[34]: &#39;O&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Training and validation split\nDivide up our training set to use 90% for training and 10% for validation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9331fcc6-1239-4501-bc48-e8a9a0763fc2"}}},{"cell_type":"code","source":["from torch.utils.data import TensorDataset, random_split\n\n\ndataset = TensorDataset(input_ids, attention_masks, labels)\n\n\ntrain_size = int(0.9 *len(dataset))\nval_size = len(dataset) - train_size\n\n\ntrain_dataset , val_dataset = random_split(dataset, [train_size, val_size])\n\nprint('{:>5,} training samples'.format(train_size))\nprint('{:>5,} validation samples'.format(val_size))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"14e8eda2-a8a8-4e67-bfd3-5b9dfdc3e53e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">7,695 training samples\n  856 validation samples\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">7,695 training samples\n  856 validation samples\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c577f82c-e7b2-476a-9717-f622a3fabb83"}}},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n\nbatch_size = 32\n\n\ntrain_dataloader = DataLoader(train_dataset,\n                             sampler= RandomSampler(train_dataset),\n                             batch_size = batch_size)\n\nvalidation_dataloader = DataLoader(val_dataset,\n                             sampler= RandomSampler(val_dataset),\n                             batch_size = batch_size)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1a0ca7ef-c932-46e5-9a80-111a21dc4d36"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Train our Classification Model\n\nNow it is time to finetune the BERT model."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"544a9656-301f-4011-9fea-5c41c6040946"}}},{"cell_type":"markdown","source":["## BertForSequenceClassification\n\nFor this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. \n\nThankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.  \n\nHere is the current list of classes provided for fine-tuning:\n* BertModel\n* BertForPreTraining\n* BertForMaskedLM\n* BertForNextSentencePrediction\n* **BertForSequenceClassification** - The one we'll use.\n* BertForTokenClassification\n* BertForQuestionAnswering\n\nThe documentation for these can be found under [here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html).\n\n\n\nWe'll be using [BertForSequenceClassification](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. \n\n\nOK, let's load BERT! There are a few different pre-trained BERT models available. \"bert-base-uncased\" means the version that has only lowercase letters (\"uncased\") and is the smaller version of the two (\"base\" vs \"large\").\n\nThe documentation for `from_pretrained` can be found [here](https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained), with the additional parameters defined [here](https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe6d4f24-6b0a-4e1a-8539-c797850ef039"}}},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7130e631-6566-45d1-8a01-f6aa6a4590b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n                                                     num_labels = 2,\n                                                     output_attentions= False,\n                                                     output_hidden_states = False)\n\nmodel.cuda()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e113811-f855-4fe1-92c7-df943aeeb892"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\rDownloading:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 570/570 [00:00&lt;00:00, 952kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00&lt;?, ?B/s]\rDownloading:   1%|          | 4.23M/440M [00:00&lt;00:10, 42.3MB/s]\rDownloading:   2%|▏         | 9.85M/440M [00:00&lt;00:09, 45.7MB/s]\rDownloading:   3%|▎         | 15.4M/440M [00:00&lt;00:08, 48.3MB/s]\rDownloading:   5%|▍         | 21.1M/440M [00:00&lt;00:08, 50.6MB/s]\rDownloading:   6%|▌         | 27.0M/440M [00:00&lt;00:07, 52.7MB/s]\rDownloading:   7%|▋         | 32.5M/440M [00:00&lt;00:07, 53.5MB/s]\rDownloading:   9%|▊         | 38.3M/440M [00:00&lt;00:07, 54.8MB/s]\rDownloading:  10%|█         | 44.1M/440M [00:00&lt;00:07, 55.6MB/s]\rDownloading:  11%|█▏        | 49.9M/440M [00:00&lt;00:06, 56.3MB/s]\rDownloading:  13%|█▎        | 55.7M/440M [00:01&lt;00:06, 56.9MB/s]\rDownloading:  14%|█▍        | 61.5M/440M [00:01&lt;00:06, 57.3MB/s]\rDownloading:  15%|█▌        | 67.4M/440M [00:01&lt;00:06, 57.6MB/s]\rDownloading:  17%|█▋        | 73.2M/440M [00:01&lt;00:06, 57.8MB/s]\rDownloading:  18%|█▊        | 78.9M/440M [00:01&lt;00:06, 57.6MB/s]\rDownloading:  19%|█▉        | 84.8M/440M [00:01&lt;00:06, 57.8MB/s]\rDownloading:  21%|██        | 90.6M/440M [00:01&lt;00:06, 57.4MB/s]\rDownloading:  22%|██▏       | 96.4M/440M [00:01&lt;00:05, 57.6MB/s]\rDownloading:  23%|██▎       | 102M/440M [00:01&lt;00:05, 57.6MB/s] \rDownloading:  24%|██▍       | 108M/440M [00:01&lt;00:05, 57.6MB/s]\rDownloading:  26%|██▌       | 114M/440M [00:02&lt;00:05, 57.6MB/s]\rDownloading:  27%|██▋       | 119M/440M [00:02&lt;00:05, 57.6MB/s]\rDownloading:  28%|██▊       | 125M/440M [00:02&lt;00:05, 56.6MB/s]\rDownloading:  30%|██▉       | 131M/440M [00:02&lt;00:05, 57.9MB/s]\rDownloading:  31%|███       | 137M/440M [00:02&lt;00:05, 58.8MB/s]\rDownloading:  33%|███▎      | 144M/440M [00:02&lt;00:04, 59.7MB/s]\rDownloading:  34%|███▍      | 150M/440M [00:02&lt;00:04, 60.3MB/s]\rDownloading:  35%|███▌      | 156M/440M [00:02&lt;00:04, 61.1MB/s]\rDownloading:  37%|███▋      | 162M/440M [00:02&lt;00:04, 61.2MB/s]\rDownloading:  38%|███▊      | 168M/440M [00:02&lt;00:04, 61.1MB/s]\rDownloading:  40%|███▉      | 174M/440M [00:03&lt;00:05, 48.1MB/s]\rDownloading:  41%|████      | 180M/440M [00:03&lt;00:05, 48.5MB/s]\rDownloading:  42%|████▏     | 185M/440M [00:03&lt;00:05, 50.7MB/s]\rDownloading:  43%|████▎     | 191M/440M [00:03&lt;00:04, 53.5MB/s]\rDownloading:  45%|████▍     | 198M/440M [00:03&lt;00:04, 55.8MB/s]\rDownloading:  46%|████▋     | 204M/440M [00:03&lt;00:04, 57.3MB/s]\rDownloading:  48%|████▊     | 210M/440M [00:03&lt;00:03, 58.6MB/s]\rDownloading:  49%|████▉     | 216M/440M [00:03&lt;00:03, 59.7MB/s]\rDownloading:  51%|█████     | 222M/440M [00:03&lt;00:03, 60.4MB/s]\rDownloading:  52%|█████▏    | 229M/440M [00:04&lt;00:03, 60.7MB/s]\rDownloading:  53%|█████▎    | 235M/440M [00:04&lt;00:03, 61.0MB/s]\rDownloading:  55%|█████▍    | 241M/440M [00:04&lt;00:03, 60.8MB/s]\rDownloading:  56%|█████▌    | 247M/440M [00:04&lt;00:03, 61.5MB/s]\rDownloading:  58%|█████▊    | 253M/440M [00:04&lt;00:03, 61.1MB/s]\rDownloading:  59%|█████▉    | 260M/440M [00:04&lt;00:02, 61.0MB/s]\rDownloading:  60%|██████    | 266M/440M [00:04&lt;00:02, 61.0MB/s]\rDownloading:  62%|██████▏   | 272M/440M [00:04&lt;00:02, 61.3MB/s]\rDownloading:  63%|██████▎   | 278M/440M [00:04&lt;00:02, 61.5MB/s]\rDownloading:  65%|██████▍   | 284M/440M [00:04&lt;00:02, 61.8MB/s]\rDownloading:  66%|██████▌   | 290M/440M [00:05&lt;00:02, 61.4MB/s]\rDownloading:  67%|██████▋   | 297M/440M [00:05&lt;00:02, 61.7MB/s]\rDownloading:  69%|██████▉   | 303M/440M [00:05&lt;00:02, 61.6MB/s]\rDownloading:  70%|███████   | 309M/440M [00:05&lt;00:02, 61.6MB/s]\rDownloading:  72%|███████▏  | 315M/440M [00:05&lt;00:02, 59.7MB/s]\rDownloading:  73%|███████▎  | 321M/440M [00:05&lt;00:02, 59.3MB/s]\rDownloading:  74%|███████▍  | 327M/440M [00:05&lt;00:01, 59.0MB/s]\rDownloading:  76%|███████▌  | 333M/440M [00:05&lt;00:01, 58.8MB/s]\rDownloading:  77%|███████▋  | 339M/440M [00:05&lt;00:01, 58.7MB/s]\rDownloading:  78%|███████▊  | 345M/440M [00:05&lt;00:01, 58.5MB/s]\rDownloading:  80%|███████▉  | 351M/440M [00:06&lt;00:01, 58.4MB/s]\rDownloading:  81%|████████  | 356M/440M [00:06&lt;00:01, 58.1MB/s]\rDownloading:  82%|████████▏ | 362M/440M [00:06&lt;00:01, 57.8MB/s]\rDownloading:  84%|████████▎ | 368M/440M [00:06&lt;00:01, 57.9MB/s]\rDownloading:  85%|████████▍ | 374M/440M [00:06&lt;00:01, 58.0MB/s]\rDownloading:  86%|████████▌ | 380M/440M [00:06&lt;00:01, 58.1MB/s]\rDownloading:  88%|████████▊ | 386M/440M [00:06&lt;00:00, 58.1MB/s]\rDownloading:  89%|████████▉ | 391M/440M [00:06&lt;00:00, 57.4MB/s]\rDownloading:  90%|█████████ | 397M/440M [00:06&lt;00:00, 57.5MB/s]\rDownloading:  91%|█████████▏| 403M/440M [00:06&lt;00:00, 57.6MB/s]\rDownloading:  93%|█████████▎| 409M/440M [00:07&lt;00:00, 58.4MB/s]\rDownloading:  94%|█████████▍| 415M/440M [00:07&lt;00:00, 58.2MB/s]\rDownloading:  96%|█████████▌| 421M/440M [00:07&lt;00:00, 58.4MB/s]\rDownloading:  97%|█████████▋| 427M/440M [00:07&lt;00:00, 56.7MB/s]\rDownloading:  98%|█████████▊| 433M/440M [00:07&lt;00:00, 57.7MB/s]\rDownloading: 100%|█████████▉| 439M/440M [00:07&lt;00:00, 59.3MB/s]\rDownloading: 100%|██████████| 440M/440M [00:07&lt;00:00, 58.2MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.bias&#39;]\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nOut[20]: BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\rDownloading:   0%|          | 0.00/570 [00:00&lt;?, ?B/s]\rDownloading: 100%|██████████| 570/570 [00:00&lt;00:00, 952kB/s]\n\rDownloading:   0%|          | 0.00/440M [00:00&lt;?, ?B/s]\rDownloading:   1%|          | 4.23M/440M [00:00&lt;00:10, 42.3MB/s]\rDownloading:   2%|▏         | 9.85M/440M [00:00&lt;00:09, 45.7MB/s]\rDownloading:   3%|▎         | 15.4M/440M [00:00&lt;00:08, 48.3MB/s]\rDownloading:   5%|▍         | 21.1M/440M [00:00&lt;00:08, 50.6MB/s]\rDownloading:   6%|▌         | 27.0M/440M [00:00&lt;00:07, 52.7MB/s]\rDownloading:   7%|▋         | 32.5M/440M [00:00&lt;00:07, 53.5MB/s]\rDownloading:   9%|▊         | 38.3M/440M [00:00&lt;00:07, 54.8MB/s]\rDownloading:  10%|█         | 44.1M/440M [00:00&lt;00:07, 55.6MB/s]\rDownloading:  11%|█▏        | 49.9M/440M [00:00&lt;00:06, 56.3MB/s]\rDownloading:  13%|█▎        | 55.7M/440M [00:01&lt;00:06, 56.9MB/s]\rDownloading:  14%|█▍        | 61.5M/440M [00:01&lt;00:06, 57.3MB/s]\rDownloading:  15%|█▌        | 67.4M/440M [00:01&lt;00:06, 57.6MB/s]\rDownloading:  17%|█▋        | 73.2M/440M [00:01&lt;00:06, 57.8MB/s]\rDownloading:  18%|█▊        | 78.9M/440M [00:01&lt;00:06, 57.6MB/s]\rDownloading:  19%|█▉        | 84.8M/440M [00:01&lt;00:06, 57.8MB/s]\rDownloading:  21%|██        | 90.6M/440M [00:01&lt;00:06, 57.4MB/s]\rDownloading:  22%|██▏       | 96.4M/440M [00:01&lt;00:05, 57.6MB/s]\rDownloading:  23%|██▎       | 102M/440M [00:01&lt;00:05, 57.6MB/s] \rDownloading:  24%|██▍       | 108M/440M [00:01&lt;00:05, 57.6MB/s]\rDownloading:  26%|██▌       | 114M/440M [00:02&lt;00:05, 57.6MB/s]\rDownloading:  27%|██▋       | 119M/440M [00:02&lt;00:05, 57.6MB/s]\rDownloading:  28%|██▊       | 125M/440M [00:02&lt;00:05, 56.6MB/s]\rDownloading:  30%|██▉       | 131M/440M [00:02&lt;00:05, 57.9MB/s]\rDownloading:  31%|███       | 137M/440M [00:02&lt;00:05, 58.8MB/s]\rDownloading:  33%|███▎      | 144M/440M [00:02&lt;00:04, 59.7MB/s]\rDownloading:  34%|███▍      | 150M/440M [00:02&lt;00:04, 60.3MB/s]\rDownloading:  35%|███▌      | 156M/440M [00:02&lt;00:04, 61.1MB/s]\rDownloading:  37%|███▋      | 162M/440M [00:02&lt;00:04, 61.2MB/s]\rDownloading:  38%|███▊      | 168M/440M [00:02&lt;00:04, 61.1MB/s]\rDownloading:  40%|███▉      | 174M/440M [00:03&lt;00:05, 48.1MB/s]\rDownloading:  41%|████      | 180M/440M [00:03&lt;00:05, 48.5MB/s]\rDownloading:  42%|████▏     | 185M/440M [00:03&lt;00:05, 50.7MB/s]\rDownloading:  43%|████▎     | 191M/440M [00:03&lt;00:04, 53.5MB/s]\rDownloading:  45%|████▍     | 198M/440M [00:03&lt;00:04, 55.8MB/s]\rDownloading:  46%|████▋     | 204M/440M [00:03&lt;00:04, 57.3MB/s]\rDownloading:  48%|████▊     | 210M/440M [00:03&lt;00:03, 58.6MB/s]\rDownloading:  49%|████▉     | 216M/440M [00:03&lt;00:03, 59.7MB/s]\rDownloading:  51%|█████     | 222M/440M [00:03&lt;00:03, 60.4MB/s]\rDownloading:  52%|█████▏    | 229M/440M [00:04&lt;00:03, 60.7MB/s]\rDownloading:  53%|█████▎    | 235M/440M [00:04&lt;00:03, 61.0MB/s]\rDownloading:  55%|█████▍    | 241M/440M [00:04&lt;00:03, 60.8MB/s]\rDownloading:  56%|█████▌    | 247M/440M [00:04&lt;00:03, 61.5MB/s]\rDownloading:  58%|█████▊    | 253M/440M [00:04&lt;00:03, 61.1MB/s]\rDownloading:  59%|█████▉    | 260M/440M [00:04&lt;00:02, 61.0MB/s]\rDownloading:  60%|██████    | 266M/440M [00:04&lt;00:02, 61.0MB/s]\rDownloading:  62%|██████▏   | 272M/440M [00:04&lt;00:02, 61.3MB/s]\rDownloading:  63%|██████▎   | 278M/440M [00:04&lt;00:02, 61.5MB/s]\rDownloading:  65%|██████▍   | 284M/440M [00:04&lt;00:02, 61.8MB/s]\rDownloading:  66%|██████▌   | 290M/440M [00:05&lt;00:02, 61.4MB/s]\rDownloading:  67%|██████▋   | 297M/440M [00:05&lt;00:02, 61.7MB/s]\rDownloading:  69%|██████▉   | 303M/440M [00:05&lt;00:02, 61.6MB/s]\rDownloading:  70%|███████   | 309M/440M [00:05&lt;00:02, 61.6MB/s]\rDownloading:  72%|███████▏  | 315M/440M [00:05&lt;00:02, 59.7MB/s]\rDownloading:  73%|███████▎  | 321M/440M [00:05&lt;00:02, 59.3MB/s]\rDownloading:  74%|███████▍  | 327M/440M [00:05&lt;00:01, 59.0MB/s]\rDownloading:  76%|███████▌  | 333M/440M [00:05&lt;00:01, 58.8MB/s]\rDownloading:  77%|███████▋  | 339M/440M [00:05&lt;00:01, 58.7MB/s]\rDownloading:  78%|███████▊  | 345M/440M [00:05&lt;00:01, 58.5MB/s]\rDownloading:  80%|███████▉  | 351M/440M [00:06&lt;00:01, 58.4MB/s]\rDownloading:  81%|████████  | 356M/440M [00:06&lt;00:01, 58.1MB/s]\rDownloading:  82%|████████▏ | 362M/440M [00:06&lt;00:01, 57.8MB/s]\rDownloading:  84%|████████▎ | 368M/440M [00:06&lt;00:01, 57.9MB/s]\rDownloading:  85%|████████▍ | 374M/440M [00:06&lt;00:01, 58.0MB/s]\rDownloading:  86%|████████▌ | 380M/440M [00:06&lt;00:01, 58.1MB/s]\rDownloading:  88%|████████▊ | 386M/440M [00:06&lt;00:00, 58.1MB/s]\rDownloading:  89%|████████▉ | 391M/440M [00:06&lt;00:00, 57.4MB/s]\rDownloading:  90%|█████████ | 397M/440M [00:06&lt;00:00, 57.5MB/s]\rDownloading:  91%|█████████▏| 403M/440M [00:06&lt;00:00, 57.6MB/s]\rDownloading:  93%|█████████▎| 409M/440M [00:07&lt;00:00, 58.4MB/s]\rDownloading:  94%|█████████▍| 415M/440M [00:07&lt;00:00, 58.2MB/s]\rDownloading:  96%|█████████▌| 421M/440M [00:07&lt;00:00, 58.4MB/s]\rDownloading:  97%|█████████▋| 427M/440M [00:07&lt;00:00, 56.7MB/s]\rDownloading:  98%|█████████▊| 433M/440M [00:07&lt;00:00, 57.7MB/s]\rDownloading: 100%|█████████▉| 439M/440M [00:07&lt;00:00, 59.3MB/s]\rDownloading: 100%|██████████| 440M/440M [00:07&lt;00:00, 58.2MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.bias&#39;]\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.bias&#39;, &#39;classifier.weight&#39;]\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nOut[20]: BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Investigate the models parameters\n\nWe can browse all of the model's parameters by name here.\n\nIn the below cell, we'll print out the names and dimensions of the weights for:\n\n1. The embedding layer.\n2. The first of the twelve transformers.\n3. The output layer."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae187837-e0e4-4df4-8e76-f6ba260c4805"}}},{"cell_type":"code","source":["#Get all of the parameters as a list of tuples\n\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('======= Embedding Layer =======\\n')\n\nfor p in params[:5]:\n  print(p[0],'\\n{}'.format(str(tuple((p[1].shape)))))\n  \nprint('======= Encoder Layer =======\\n')\n\nfor p in params[5:21]:\n  print(p[0],'\\n',tuple(p[1].size()))\n  \nprint('======= Output Layer =======\\n')\n\nfor p in params[197:]:\n  print(p[0],'\\n',tuple(p[1].size()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76c33f1c-7178-438c-bb50-f30795188674"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">The BERT model has 201 different named parameters.\n\n======= Embedding Layer =======\n\nbert.embeddings.word_embeddings.weight \n(30522, 768)\nbert.embeddings.position_embeddings.weight \n(512, 768)\nbert.embeddings.token_type_embeddings.weight \n(2, 768)\nbert.embeddings.LayerNorm.weight \n(768,)\nbert.embeddings.LayerNorm.bias \n(768,)\n======= Encoder Layer =======\n\nbert.encoder.layer.0.attention.self.query.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.query.bias \n (768,)\nbert.encoder.layer.0.attention.self.key.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.key.bias \n (768,)\nbert.encoder.layer.0.attention.self.value.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.value.bias \n (768,)\nbert.encoder.layer.0.attention.output.dense.weight \n (768, 768)\nbert.encoder.layer.0.attention.output.dense.bias \n (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight \n (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias \n (768,)\nbert.encoder.layer.0.intermediate.dense.weight \n (3072, 768)\nbert.encoder.layer.0.intermediate.dense.bias \n (3072,)\nbert.encoder.layer.0.output.dense.weight \n (768, 3072)\nbert.encoder.layer.0.output.dense.bias \n (768,)\nbert.encoder.layer.0.output.LayerNorm.weight \n (768,)\nbert.encoder.layer.0.output.LayerNorm.bias \n (768,)\n======= Output Layer =======\n\nbert.pooler.dense.weight \n (768, 768)\nbert.pooler.dense.bias \n (768,)\nclassifier.weight \n (2, 768)\nclassifier.bias \n (2,)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The BERT model has 201 different named parameters.\n\n======= Embedding Layer =======\n\nbert.embeddings.word_embeddings.weight \n(30522, 768)\nbert.embeddings.position_embeddings.weight \n(512, 768)\nbert.embeddings.token_type_embeddings.weight \n(2, 768)\nbert.embeddings.LayerNorm.weight \n(768,)\nbert.embeddings.LayerNorm.bias \n(768,)\n======= Encoder Layer =======\n\nbert.encoder.layer.0.attention.self.query.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.query.bias \n (768,)\nbert.encoder.layer.0.attention.self.key.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.key.bias \n (768,)\nbert.encoder.layer.0.attention.self.value.weight \n (768, 768)\nbert.encoder.layer.0.attention.self.value.bias \n (768,)\nbert.encoder.layer.0.attention.output.dense.weight \n (768, 768)\nbert.encoder.layer.0.attention.output.dense.bias \n (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.weight \n (768,)\nbert.encoder.layer.0.attention.output.LayerNorm.bias \n (768,)\nbert.encoder.layer.0.intermediate.dense.weight \n (3072, 768)\nbert.encoder.layer.0.intermediate.dense.bias \n (3072,)\nbert.encoder.layer.0.output.dense.weight \n (768, 3072)\nbert.encoder.layer.0.output.dense.bias \n (768,)\nbert.encoder.layer.0.output.LayerNorm.weight \n (768,)\nbert.encoder.layer.0.output.LayerNorm.bias \n (768,)\n======= Output Layer =======\n\nbert.pooler.dense.weight \n (768, 768)\nbert.pooler.dense.bias \n (768,)\nclassifier.weight \n (2, 768)\nclassifier.bias \n (2,)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Optimizer and Learning Rate\n\nNow that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n\nFor the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n\n>- **Batch size:** 16, 32  \n- **Learning rate (Adam):** 5e-5, 3e-5, 2e-5  \n- **Number of epochs:** 2, 3, 4 \n\nWe chose:\n* Batch size: 32 (set when creating our DataLoaders)\n* Learning rate: 2e-5\n* Epochs: 4 (we'll see that this is probably too many...)\n\nThe epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)).\n\nYou can find the creation of the AdamW optimizer in `run_glue.py` [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5421e9cb-7599-475d-8525-33971accccf6"}}},{"cell_type":"code","source":["optimizer = AdamW(model.parameters(),\n                 lr= 2e-5,\n                 eps = 1e-8 )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eaee66e2-84ed-4916-a7a1-fb2791b8ab1b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from transformers import get_linear_schedule_with_warmup\n\n\nepochs = 4\n\n\ntotal_steps = len(train_dataloader) * epochs\n\n\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                           num_warmup_steps= 0,\n                                           num_training_steps= total_steps)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97e92dda-c913-4134-bac8-2b29a01e84dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Training Loop\n\n**Training:**\n- Unpack our data inputs and labels\n- Load data onto the GPU for acceleration\n- Clear out the gradients calculated in the previous pass. \n    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n- Forward pass (feed input data through the network)\n- Backward pass (backpropagation)\n- Tell the network to update parameters with optimizer.step()\n- Track variables for monitoring progress\n\n**Evalution:**\n- Unpack our data inputs and labels\n- Load data onto the GPU for acceleration\n- Forward pass (feed input data through the network)\n- Compute loss on our validation data and track variables for monitoring progress"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e99257f1-5e12-4389-a446-14acfa458afa"}}},{"cell_type":"code","source":["import numpy as np\n\ndef flat_accuracy(preds, labels):\n  pred_flat = np.argmax(preds, axis=1).flatten()\n  labels_flat = labels.flatten()\n  \n  return np.sum(pred_flat == labels_flat)/len(labels_flat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bb7049a-7365-4d5b-b128-6c1ecbc9cd50"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import time\nimport datetime\n\ndef format_time(elapsed):\n  \n  elapsed_rounded = int(round(elapsed))\n  \n  return str(datetime.timedelta(seconds = elapsed_rounded))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fe88730-213c-4e90-83fa-c2a250b2b56c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import random\nimport numpy as np\n\n# This training code is based on the `run_glue.py` script here:\n# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# We'll store a number of quantities such as training and validation loss, \n# validation accuracy, and timings.\ntraining_stats = []\n\n# Measure the total training time for the whole run.\ntotal_t0 = time.time()\n\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_train_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # In PyTorch, calling `model` will in turn call the model's `forward` \n        # function and pass down the arguments. The `forward` function is \n        # documented here: \n        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n        # The results are returned in a results object, documented here:\n        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n        # Specifically, we'll get the loss (because we provided labels) and the\n        # \"logits\"--the model outputs prior to activation.\n        result = model(b_input_ids, \n                       token_type_ids=None, \n                       attention_mask=b_input_mask, \n                       labels=b_labels,\n                       return_dict=True)\n\n        loss = result.loss\n        logits = result.logits\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)            \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(training_time))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using \n        # the `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            result = model(b_input_ids, \n                           token_type_ids=None, \n                           attention_mask=b_input_mask,\n                           labels=b_labels,\n                           return_dict=True)\n\n        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n        # output values prior to applying an activation function like the \n        # softmax.\n        loss = result.loss\n        logits = result.logits\n            \n        # Accumulate the validation loss.\n        total_eval_loss += loss.item()\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        # Calculate the accuracy for this batch of test sentences, and\n        # accumulate it over all batches.\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n\n    # Report the final accuracy for this validation run.\n    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n\n    # Calculate the average loss over all of the batches.\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n    # Measure how long the validation run took.\n    validation_time = format_time(time.time() - t0)\n    \n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Validation took: {:}\".format(validation_time))\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Valid. Accur.': avg_val_accuracy,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(\"\")\nprint(\"Training complete!\")\n\nprint(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"157449f1-cb75-4755-92b5-e5db73f1f409"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">\n======== Epoch 1 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:27.\n  Batch    80  of    241.    Elapsed: 0:00:55.\n  Batch   120  of    241.    Elapsed: 0:01:23.\n  Batch   160  of    241.    Elapsed: 0:01:51.\n  Batch   200  of    241.    Elapsed: 0:02:19.\n  Batch   240  of    241.    Elapsed: 0:02:46.\n\n  Average training loss: 0.08\n  Training epcoh took: 0:02:47\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\n======== Epoch 2 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:49.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.58\n  Validation took: 0:00:06\n\n======== Epoch 3 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:48.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\n======== Epoch 4 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:48.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\nTraining complete!\nTotal training took 0:11:39 (h:mm:ss)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n======== Epoch 1 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:27.\n  Batch    80  of    241.    Elapsed: 0:00:55.\n  Batch   120  of    241.    Elapsed: 0:01:23.\n  Batch   160  of    241.    Elapsed: 0:01:51.\n  Batch   200  of    241.    Elapsed: 0:02:19.\n  Batch   240  of    241.    Elapsed: 0:02:46.\n\n  Average training loss: 0.08\n  Training epcoh took: 0:02:47\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\n======== Epoch 2 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:49.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.58\n  Validation took: 0:00:06\n\n======== Epoch 3 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:48.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\n======== Epoch 4 / 4 ========\nTraining...\n  Batch    40  of    241.    Elapsed: 0:00:28.\n  Batch    80  of    241.    Elapsed: 0:00:56.\n  Batch   120  of    241.    Elapsed: 0:01:24.\n  Batch   160  of    241.    Elapsed: 0:01:52.\n  Batch   200  of    241.    Elapsed: 0:02:20.\n  Batch   240  of    241.    Elapsed: 0:02:48.\n\n  Average training loss: 0.12\n  Training epcoh took: 0:02:49\n\nRunning Validation...\n  Accuracy: 0.82\n  Validation Loss: 0.57\n  Validation took: 0:00:06\n\nTraining complete!\nTotal training took 0:11:39 (h:mm:ss)\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Review of the summary of the training process"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ad6855a-c969-4022-bbae-bd679d3f245a"}}},{"cell_type":"code","source":["import pandas as pd\n\n# Display floats with two decimal places.\npd.set_option('precision', 2)\n\n# Create a DataFrame from our training statistics.\ndf_stats = pd.DataFrame(data=training_stats)\n\n# Use the 'epoch' as the row index.\ndf_stats = df_stats.set_index('epoch')\n\n# A hack to force the column headers to wrap.\n#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n\n# Display the table.\ndf_stats"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eeb7d9f3-238a-4b22-bd9d-53b522f812b5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.08</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:47</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.12</td>\n      <td>0.58</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.12</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.12</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.08</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:47</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.12</td>\n      <td>0.58</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.12</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.12</td>\n      <td>0.57</td>\n      <td>0.82</td>\n      <td>0:02:49</td>\n      <td>0:00:06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it's over-fitting on the training data. \n\n(For reference, we are using 7,695 training samples and 856 validation samples).\n\nValidation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on. \n\nIf we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8cac725b-e980-4507-8e69-fa799f4c0524"}}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n#% matplotlib inline\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\n# Label the plot.\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.xticks([1, 2, 3, 4])\n\nplt.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45c42ff4-39fb-4f29-bb92-98ab24d7b998"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/plots/ed85a6a4-166d-41e2-9af3-6a17f1e9cd16.png","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"image","arguments":{}}},"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAuUAAAGXCAYAAAAUOC6pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde1xUZeLH8e8Ml/ECAqKlK5rmBLmK17ylvyi8ppXXss3U0jJTNrey3fqZu91stYu2xpJWrllpZmiUpramxv7M0jXTtrzrZlhahsAAKrc5vz+AgWEGHBA4mJ/362UxzznPZQ4e+Z6H55yxGIZhCAAAAIBprGYPAAAAALjUEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygH8ah0/flxRUVF6+eWXq9zGo48+qqioqGoc1a9Xecc7KipKjz76qE9tvPzyy4qKitLx48erfXyrV69WVFSUtm/fXu1tA8CF8jd7AAAuHZUJt5s2bVJEREQNjubic+bMGS1cuFDr1q3Tzz//rMaNG6tbt26aOnWq2rZt61MbDzzwgD7++GMlJSWpXbt2XvcxDEP9+vWTw+HQ1q1bVa9evep8GzVq+/bt2rFjhyZMmKBGjRqZPRwPx48fV79+/TR27Fj9+c9/Nns4AOoQQjmAWvPcc8+5vf7yyy/17rvvasyYMerWrZvbtsaNG19wfy1atNDXX38tPz+/Krfx9NNP68knn7zgsVSHxx9/XB999JFuuukm9ejRQ6dOndLmzZu1Z88en0P56NGj9fHHH2vVqlV6/PHHve7zxRdf6IcfftCYMWOqJZB//fXXslpr5xezO3bsUHx8vEaMGOERyocNG6ahQ4cqICCgVsYCAJVBKAdQa4YNG+b2uqCgQO+++646d+7ssa2srKwsBQUFVao/i8Uim81W6XGWVlcC3NmzZ7Vhwwb17dtXL774oqs8Li5Oubm5PrfTt29fNW/eXGvWrNEf//hHBQYGeuyzevVqSYUBvjpc6Peguvj5+V3QBRoA1CTWlAOoc2JjYzVu3Djt3btXkyZNUrdu3XTLLbdIKgzn8+fP16233qqePXuqQ4cOGjBggF544QWdPXvWrR1va5xLl23ZskWjRo1SdHS0+vbtq7lz5yo/P9+tDW9ryovLMjMz9Ze//EW9e/dWdHS0br/9du3Zs8fj/aSlpemxxx5Tz5491aVLF40fP1579+7VuHHjFBsb69MxsVgsslgsXrd5C9blsVqtGjFihNLT07V582aP7VlZWdq4caMiIyPVsWPHSh3v8nhbU+50OrVo0SLFxsYqOjpaN998sz788EOv9Y8cOaInnnhCQ4cOVZcuXdSpUyeNHDlSK1eudNvv0UcfVXx8vCSpX79+ioqKcvv+l7em/PTp03ryyScVExOjDh06KCYmRk8++aTS0tLc9iuu//nnn2vx4sXq37+/OnTooEGDBun999/36VhUxv79+zVt2jT17NlT0dHRGjJkiF577TUVFBS47XfixAk99thjuuGGG9ShQwf17t1bt99+u9uYDMPQG2+8oZtvvlldunRR165dNWjQIP3v//6v8vLyqn3sACqPmXIAddKPP/6oCRMmaPDgwRo4cKDOnDkjSfrpp5+UmJiogQMH6qabbpK/v7927Nih119/Xfv27dPixYt9aj85OVnLly/X7bffrlGjRmnTpk36xz/+oZCQEE2ZMsWnNiZNmqTGjRtr2rRpSk9P15IlSzR58mRt2rTJNaufm5uru+++W/v27dPIkSMVHR2tAwcO6O6771ZISIjPx6NevXoaPny4EhMTtXbtWt10000+1y1r5MiReuWVV7R69WoNHjzYbdtHH32ks2fPatSoUZKq73iX9de//lVvvvmmunfvrrvuukupqal66qmn1LJlS499d+zYoZ07d+r6669XRESE67cGs2bNUlpamu677z5J0pgxY1wXFY899pjCwsIkVXwvQ2Zmpn73u9/p2LFjGjVqlH77299q3759euedd/TFF1/ovffe8/gNzfz583Xu3DmNGTNGgYGBeuedd/Too4+qVatWHsuwquo///mPxo0bJ39/f40dO1ZNmjTRli1b9MILL2j//v2u35bk5+fr7rvv1k8//aQ77rhDrVu3VlZWlg4cOKCdO3dqxIgRkqSEhAQtWLBAN9xwg26//Xb5+fnp+PHj2rx5s3Jzc+vMb4SAS5oBACZZtWqVERkZaaxatcqt/IYbbjAiIyONlStXetTJyckxcnNzPcrnz59vREZGGnv27HGVpaSkGJGRkcaCBQs8yjp16mSkpKS4yp1OpzF06FCjT58+bu3+6U9/MiIjI72W/eUvf3ErX7dunREZGWm88847rrK3337biIyMNBISEtz2LS6/4YYbPN6LN5mZmca9995rdOjQwfjtb39rfPTRRz7VK8/48eONdu3aGSdPnnQrv+2224z27dsbqamphmFc+PE2DMOIjIw0/vSnP7leHzlyxIiKijLGjx9v5Ofnu8q/+eYbIyoqyoiMjHT73mRnZ3v0X1BQYNx5551G165d3ca3YMECj/rFiv++ffHFF66yefPmGZGRkcbbb7/ttm/x92f+/Pke9YcNG2bk5OS4yk+ePGm0b9/eePDBBz36LKv4GD355JMV7jdmzBijXbt2xr59+1xlTqfTeOCBB4zIyEhj27ZthmEYxr59+4zIyEjj1VdfrbC94cOHGzfeeON5xwfAPCxfAVAnhYaGauTIkR7lgYGBrlm9/Px8ZWRk6PTp07r22mslyevyEW/69evn9nQXi8Winj176tSpU8rOzvapjbvuusvtda9evSRJx44dc5Vt2bJFfn5+Gj9+vNu+t912m4KDg33qx+l0avr06dq/f7/Wr1+v6667TjNmzNCaNWvc9ps1a5bat2/v0xrz0aNHq6CgQB988IGr7MiRI9q9e7diY2NdN9pW1/EubdOmTTIMQ3fffbfbGu/27durT58+Hvs3aNDA9XVOTo7S0tKUnp6uPn36KCsrS0ePHq30GIpt3LhRjRs31pgxY9zKx4wZo7CwMH3yyScede644w63JUOXX3652rRpo++++67K4ygtNTVVX331lWJjY3X11Ve7yi0Wi+u3OBs3bpQk19+h7du3KzU1tdw2g4KC9NNPP2nnzp3VMkYA1Y/lKwDqpJYtW5Z7U96yZcu0YsUKHT58WE6n021bRkaGz+2XFRoaKklKT09Xw4YNK91G8XKJ9PR0V9nx48d12WWXebQXEBCgiIgIORyO8/azadMmbd26Vc8//7wiIiL0t7/9Tb///e/1xz/+Ufn5+a4lCgcOHFB0dLRPa8wHDhyoRo0aafXq1Zo8ebIkadWqVZLkWrpSrDqOd2kpKSmSpCuvvNJjW9u2bbV161a3suzsbMXHx2v9+vU6ceKERx1fjmF5jh8/rg4dOsjf3/3Hob+/v9q0aaO9e/d61Cnv784PP/xQ5XGUHZMk2e12j21t27aV1Wp1HcMWLVpoypQpevXVV9W3b1+1a9dOvXr10uDBg9WxY0dXvYceekjTpk3T2LFjddlll6lHjx66/vrrNWjQoErdkwCg5hDKAdRJ9evX91q+ZMkSzZkzR3379tX48eN12WWXKSAgQD/99JMeffRRGYbhU/sVPYXjQtsoXd/XtipSfGNi9+7dJRXOXr/88su6//779dhjjyk/P19XX3219uzZo9mzZ/vUps1m00033aTly5dr165d6tSpkz788EM1a9ZMffv2de1XXcfbG283rnpr7+GHH9ann36q2267Td27d1dISIj8/f2VnJysN954w+NCoabV9OMdK3tMH3zwQY0ePVqffvqpdu7cqcTERC1evFj33HOPHnnkEUlSly5dtHHjRm3dulXbt2/X9u3btXbtWr3yyitavny564IUgHkI5QAuKh988IFatGih1157zS0c/etf/zJxVOWLiIjQ559/ruzsbLfZ8ry8PB0/ftynD7gpfp8//PCDmjdvLqkwmCckJGjKlCmaNWuWWrRoocjISA0fPtznsY0ePVrLly/X6tWrlZGRoVOnTmnKlCluFxs1cbyLZ5qPHDniMetcdimKw+HQp59+qmHDhumpp55y27Zt2zaPtst7Qk1FY/nvf/+r/Px8t9ny/Px8fffdd15nxWtacZ+HDx/22Hb06FE5nU6PcbVs2VLjxo3TuHHjlJOTo0mTJun111/XxIkTFR4eLklq2LChBg0apEGDBkkq/A3IU089pcTERN1zzz01/K4AnA9rygFcVKxWqywWi9tsYn5+vl577TUTR1W+2NhYFRQU6M0333QrX7lypTIzM31qIyYmRpL00ksvua0Xt9lsmjdvnho1aqTjx49r0KBBHsswKtK+fXu1a9dO69at09tvvy2LxeKxdKUmjndsbKwsFouWLFni9ni/b7/91iNoF18IlJ09/vnnn/Xee+95tF28/tzXZTX9+/fX6dOnPdpauXKlTp8+rf79+/vUTnUKDw9Xly5dtGXLFh08eNBVbhiGXn31VUnSgAEDJBU+PabsIw1tNptraVDxcTh9+rRHP+3bt3fbB4C5mCkHcFEZPHiwXnzxRd17770aMGCAsrKytHbt2kqF0dp06623asWKFXrppZf0/fffux6JuGHDBl1xxRUez0X3pk+fPho9erQSExM1dOhQDRs2TM2aNVNKSorrRs327dvr73//u9q2basbb7zR5/GNHj1aTz/9tLZu3aoePXqoVatWbttr4ni3bdtWY8eO1dtvv60JEyZo4MCBSk1N1bJly3T11Ve7reMOCgpSnz599OGHH6pevXqKjo7WDz/8oHfffVcRERFu6/clqVOnTpKkF154QTfffLNsNpuuuuoqRUZGeh3LPffcow0bNuipp57S3r171a5dO+3bt0+JiYlq06ZNjc0gf/PNN0pISPAo9/f31+TJkzVz5kyNGzdOY8eO1R133KGmTZtqy5Yt2rp1q2666Sb17t1bUuHSplmzZmngwIFq06aNGjZsqG+++UaJiYnq1KmTK5wPGTJEnTt3VseOHXXZZZfp1KlTWrlypQICAjR06NAaeY8AKqdu/hQDgHJMmjRJhmEoMTFRs2fPVtOmTXXjjTdq1KhRGjJkiNnD8xAYGKilS5fqueee06ZNm7R+/Xp17NhRb7zxhmbOnKlz58751M7s2bPVo0cPrVixQosXL1ZeXp5atGihwYMHa+LEiQoMDNSYMWP0yCOPKCgoSP/zP//jU7s333yznnvuOeXk5HjMkks1d7xnzpypJk2aaOXKlXruuefUunVr/fnPf9axY8c8bq58/vnn9eKLL2rz5s16//331bp1az344IPy9/fXY4895rZvt27dNGPGDK1YsUKzZs1Sfn6+4uLiyg3lwcHBeuedd7RgwQJt3rxZq1evVnh4uG6//Xb9/ve/r/SnyPpqz549Xp9cExgYqMmTJys6OlorVqzQggUL9M477+jMmTNq2bKlZsyYoYkTJ7r2j4qK0oABA7Rjxw6tWbNGTqdTzZs313333ee238SJE5WcnKy33npLmZmZCg8PV6dOnXTfffe5PeEFgHksRnXchQQAqJSCggL16tVLHTt2rPIH8AAAfj1YUw4ANczbbPiKFSvkcDi8PpcbAHDpYfkKANSwxx9/XLm5uerSpYsCAwP11Vdfae3atbriiit02223mT08AEAdwPIVAKhhSUlJWrZsmb777judOXNG4eHhiomJ0fTp09WkSROzhwcAqAMI5QAAAIDJWFMOAAAAmIxQDgAAAJiMGz2LpKVly+ms3ZU84eFBSk3NqtU+gYsR5wrgG84VwDdmnStWq0VhYQ29biOUF3E6jVoP5cX9Ajg/zhXAN5wrgG/q2rnC8hUAAADAZIRyAAAAwGSEcgAAAMBkhHIAAADAZIRyAAAAwGSEcgAAAMBkhHIAAADAZIRyAAAAwGSEcgAAAMBkfKKnCXac3KUPj2xQek66Qm2huqXtYPVo1tXsYQF1DucKAOBSQSivZTtO7tLy/auU58yTJKXlpGv5/lWSRNgASuFcAQBUt7o82UMor2UfHtngChnF8px5WnVojRr415ckGTJc2wzDKCor81+3cm91KrNvSblRsqH0XiX1DC/7ylBJMyV9GSU7l93bo4+KxlS2Dbf/GqX3LO89u79/X/st3bfHvhUci5LuvR2LCz9u7m1XfNy8ff/dankcC+/j9/aefe+3zHE1yu5dqq9S/X6f+YMKjAKVlufM07J9ifrixE5ZLVZZLBZZZZXVUvzHUliuwq/9LFZZSm8rta+laF+rLLJa/ErqusrLtFm0r0ebxV+rVN0yfVosZcajMnXd3kuZ8cgii8UiAMCFqeuTPYTyWpaWk+61PCsvW698vaSWR3PpsMg91JQOOZbSWy2l93QvLywpXWpRSTNupcVfyGtrpcrd/mvx2rOr3OO/buVFdSyl659n30oeC299uNW0lN239Hsr3t/i+rpsH2XHVDaQF8s38pXnzJfTcMppOGUYTjlluF4X/il8bchQgVEgwzBKyuUsVdf9kqqusrhdDHheNLguEmSR1VqyzVLmoqH0BYafx0VNyb4WLxcNJRcYFvlZ/Mpc1Hi72OGipjbV5dm/X6PifztK/98pQ0bRvzslr933K/3vjtNbG6Xqe9/H6VZWtk/3Ok6v43DK6WXc5Y/Taxtl+vTcx+kx9sLX5b3/MmMt572V7afsOMvuU3acmblZHv/m5znz9OGRDXXifCGU17IwW6jXYN4oMFj3dZwgyUuQKydolf1BUzbUnC9clg5a3sJl6aBVUYircN/zBtGyJcVtnT+InjdoXwI/iH/NHv/sWa/nSpgtVA93m1pt/ZQE9pJwX/wDpcBwypB70HcF+nIvBErV9dJuQfEPSsNZdIFQ0k55FxjF5V4vMJxO1w9Zt3pl3ktJuVMFznzlucZSuk9DTqPA7aLG4z3K/X1cqhc1ZfctbsfP46Km1EWCtwsPeblQ8nLhYSm1b9nfDB1KO6rk458pv+hCNi0nXcv2vacfMk/oqrArLyzUyPv20qHJW6hzq+slsHmGJ/fA5q2d0oGtvLBX7ljOE5a9h92KQmTd/3tfE4p/tlpV+Pe95GuLa5vHPrIUnh/F2y0WWeRet6QNq3sbRV9brX7l91Pq4tvbPqW3f/bjdq/vq7wJ09pGKK9lt7Qd7ParE0kKsAZohH2oWjdqZeLIgLqlvHPllraDq7Wf4plfP/lVa7uXipILjDIXF2V+K+EK+sWB3i3wl6pb5kKlrl7UGIZTBflVv6gpvU9NyDcK9ElKsj5JSa6R9r0pPxgVX7R4D2xudbwEtpJQZ/USuIovuPzcwpe3sRRfaFVtn/ICqNVj/GXfm+d7tJ43yFq9bHMPt+XtU1Ju9TJOV3mZfb3242WcF7u9qQfKneypCwjltaz41yP8mhGoGOfKxaHkogZVUfqipuSio+KLmrIXHs/tfLnc9h+5Js5rYCsv7JYNz66g6jUYugdZoK6rrcmeqiKUm6BHs67q0ayrmjYN1qlTmWYPB6izOFfwa1cdFzXlLYsMs4XyG1iglLo+2UMoBwDgIlbXZ/+AuqQuT/YQygEAuIjV9dk/AL4hlAMAcJGry7N/AHxjNXsAAAAAwKWOUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJjM38zOs7OzNX/+fG3YsEEOh0N2u13Tpk1Tv379Kqz38ssvKz4+3qO8SZMm+uyzz2pquAAAAECNMDWUx8XFae/evZoxY4YiIiL0/vvvKy4uTgsXLlRMTMx56y9ZskQNGjRwvQ4ICKjJ4QIAAAA1wrRQnpycrG3btik+Pl4DBgyQJPXq1UspKSmaM2eOT6G8Q4cOatSoUU0PFQAAAKhRpq0p37hxo4KDg92WqlgsFo0YMUJHjx7V4cOHzRoaAAAAUKtMC+WHDh2S3W6X1eo+hKioKEnSwYMHz9vGkCFD1K5dO/Xt21ePP/64UlNTa2SsAAAAQE0ybflKenq6Wrdu7VEeEhLi2l6eli1b6qGHHlK7du0UEBCgXbt26fXXX9fnn3+u1atXu9oAAAAALgam3uhpsViqtG348OFur3v37q3OnTtr4sSJWrZsmaZOnVrpsYSHB1W6TnVo2jTYlH6Biw3nCuAbzhXAN3XtXDEtlIeGhnqdDc/IyJCkSs929+nTR02bNtXu3burNJ7U1Cw5nUaV6lZV06bBOnUqs1b7BC5GnCuAbzhXAN+Yda5YrZZyJ4JNW1Nut9t15MgROZ1Ot/LiteSRkZGVbtMwDI816gAAAEBdZ1qCHTBggBwOhzZv3uxWnpSUpDZt2shut1eqva1bt+qXX35Rp06dqnOYAAAAQI0zbflKTEyMevbsqZkzZyo9PV0RERFKSkrSl19+qYSEBNd+48aN044dO3TgwAFX2fDhwzV8+HC1adNG/v7++uqrr7R48WJdccUVGjt2rBlvBwAAAKgy00K5xWJRQkKC5s2bp/nz58vhcMhutys+Pl6xsbEV1r3yyiu1fPly/fzzz8rPz1ezZs106623aurUqXyYEAAAAC46FsMwavfuxjqKGz2BuotzBfAN5wrgG270BAAAAOCBUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmMzUUJ6dna1nnnlGffv2VceOHTVy5Eht2rSpUm0YhqHx48crKipKs2fPrqGRAgAAADXH1FAeFxenNWvWaPr06Vq0aJHsdrvi4uKUnJzscxsrV67U0aNHa3CUAAAAQM0yLZQnJydr27ZteuaZZ3Trrbeqd+/emjt3rjp37qw5c+b41MZPP/2k559/XrNmzarh0QIAAAA1x7RQvnHjRgUHB6tfv36uMovFohEjRujo0aM6fPjwedv4y1/+omuuuUaDBg2qyaECAAAANcrfrI4PHToku90uq9X9uiAqKkqSdPDgQdnt9nLrr127Vtu3b9e6detqdJwAAABATTNtpjw9PV0hISEe5cVl6enp5dY9ffq0Zs+erQcffFDNmzevsTECAAAAtcG0mXKpcLlKVbbNnj1bERERuvPOO6ttLOHhQdXWVmU0bRpsSr/AxYZzBfAN5wrgm7p2rpgWykNDQ73OhmdkZEiS11l0Sfrss8+0bt06LV26VFlZWW7bcnNz5XA41KBBA/n7V+6tpaZmyek0KlXnQjVtGqxTpzJrtU/gYsS5AviGcwXwjVnnitVqKXci2LTlK3a7XUeOHJHT6XQrP3jwoCQpMjLSa71Dhw7J6XRq3Lhx6t69u+uPJK1YsULdu3fXtm3banbwAAAAQDUybaZ8wIABSkxM1ObNm9W/f39XeVJSktq0aVPuTZ6DBw9Wu3btPMrHjx+vQYMGaezYsa6bRQEAAICLgWmhPCYmRj179tTMmTOVnp6uiIgIJSUl6csvv1RCQoJrv3HjxmnHjh06cOCAJKlZs2Zq1qyZ1zYvv/xy9ezZs1bGDwAAAFQX00K5xWJRQkKC5s2bp/nz58vhcMhutys+Pl6xsbFmDQsAAACodRbDMGr37sY6ihs9gbqLcwXwDecK4Btu9AQAAADggVAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJjM3+wBAAAA1GVnz2YrKytDBQV5Zg8F1eTnn61yOp3V1p6fX4CCgkJUv37DKrdBKAcAAChHXl6uMjPTFBraRAEBNlksFrOHhGrg729Vfn71hHLDMJSXl6P09F/k7x+ggIDAKrXD8hUAAIByZGamKygoRIGB9Qjk8MpisSgwsJ4aNgxRVlZ6ldshlAMAAJQjPz9XNlt9s4eBi0C9evWVl5db5fqEcgAAgHI4nQWyWv3MHgYuAlarn5zOgqrXr8axAAAA/OqwbAW+uNC/J4RyAAAAwGQ8fQUAAOAS0bfvNT7t9957H6p5899UuZ+4uMmSpPj4V2u17sWMUA4AAHCJWLhwSZnXLysl5Zhmz37BrTw8vMkF9fPww4+aUvdiRigHAAC4RHToEO32Ojg4WAEBgR7lZeXm5iow0Pfnb7dpc2WVxnehdS9m1RLK8/PztWnTJmVkZOiGG25Q06ZNq6NZAACAX53Pvz2p1clHlOrIUXgjm0bGtFXv9s3MHpZLXNxkZWVladq06Vq06O86evSwxo6doEmT7tMnn3ystWs/0NGjR5SdnaXmzVuof/+BuuOO8W6hvewSlF27duqBB6boySf/qoMH92vDhrU6e/ac2rVrr4cf/qNatWpdLXUNw9Bbby3RBx+sVlraabVu3Ub33jtVy5YtdWuzLqp0KH/uuee0fft2rVq1SlLhm7/77ru1c+dOGYah0NBQrVy5Uq1atar2wQIAAFzMPv/2pJau36/cok+TTHXkaOn6/ZJUp4L5qVM/ac6cpzV+/ES1bNlKDRo0kCT98MNx9elzncaMGSubzaYjRw5r6dLFSkk5plmznj5vu9TX0kUAACAASURBVAsXvqyOHTvr0UdnKSsrS6+88rL++MeHtGzZe/Lzq/jRk77UffXVBL311hINHz5a//M/Mfr555/0/PPPqqCgQC1b1u1sWulQ/n//93+69tprXa83b96sf//737rnnnvUrl07Pf3003r11Vf1zDPPVOtAAQAA6oLP/nNCW78+UaW6R37MUH6B4VaWm+/UknX79K/dP1aqrb4dm6tPdPMqjeN8MjIy9Ne/vqiOHTu7lU+YMMn1tWEY6tixs4KDg/Xss09q+vQZatQopMJ227a1a9asp1yv/fz89ec/P6p9+75Vhw4dL6iuw5Ghd99dpoEDb9SMGSXr0tu0aaspU+7+9YXykydP6oorrnC93rJliyIiIjRjxgxJ0qFDh7RmzZrqGyEAAMCvRNlAfr5ys4SGhnkEckk6fjxFb7zxunbt2qnU1F9UUFDyYTkpKSlq377iUN6373Vur+12uyTp5MkT5w3l56v77bf/UW5urmJj+7vt16FD9AU9Saa2VDqU5+Xluf16Yfv27W4z5y1bttSpU6eqZ3QAAAB1TJ/oqs9QP5LwmVIdOR7l4Y1s+tPYrhc6tGrj7ekr2dlZmjbtHtWv30ATJ05Wy5atZLPZtHfvt5o3b65ycs6dt91GjULdXgcEFK5Dz809/8fTn6+uw+GQJIWFhXvUDQtrfN72zVbpDw9q1qyZdu/eLalwVjwlJUXdu3d3bU9NTXWtOwIAAECJkTFtFejvHr8C/a0aGdPWpBF55+3TKQtnx1P16KOzdNNNw9SpUxddffVvFRgYYMIIPRUvnUlLS/XYlpZ2uraHU2mVnikfOnSoEhISdPr0aR06dEhBQUGKiYlxbd+3bx83eQIAAHhRfDNnXX76SnmKg7q/f0kINwxDa9d+aNaQ3LRv30GBgYHavPkT9e1bkk2/+eY/OnHiR11+ed0+xpUO5ffdd59OnDihTZs2KSgoSHPnzlWjRo0kSZmZmdq8ebPuuuuu6h4nAADAr0Lv9s0uihBeVocOnRQUFKwXXvirJk2aLIvFoqSkVUpPTzN7aJIKZ8rHjBmrt95aogYNGuq6667Xzz+f1D/+8ZrCw5vIaq30ApFaVelQHhgYqGeffdbrtoYNG2rr1q2qV6/eBQ8MAAAAdUdoaKjmzp2vv//9JT3xxEwFBQWpf/9BGjVqjB55ZLrZw5MkTZ48VfXq1dMHH6zWRx99oFatWmvGjMf06qsJatgwyOzhVchiGEa13e5b2U97qktSU7PkdNbunc9Nmwbr1KnMWu0TuBhxrgC+4VypfidPHlOzZlecf0fUWT/++IPGjh2tu+66x/VIR39/q/KLnhVfnc7398VqtSg83PvFQaVnypOTk/X111/r97//vats2bJlevHFF3Xu3DndeOONmjNnjgIC6saifwAAAFwaDhzYr08/3aQOHTqqfv36+v77Y1q+/E01bNhQN9883OzhVajSoXzx4sUKDy951MyRI0f07LPPqmXLloqIiNC6desUHR3NunIAAADUqvr162vv3m/04YerlZWVpaCgIHXp0k2TJ09V48aej0qsSyodyo8ePer2tJV169bJZrMpMTFRQUFBevjhh5WUlEQoBwAAQK1q1eoK/e1vr5g9jCqp9G2oGRkZCgsLc73etm2bevXqpaCgwvUxPXr00PHjx6tvhAAAAMCvXKVDeVhYmH788UdJUlZWlv7zn/+oW7duru35+fluH7kKAAAAoGKVXr7SuXNnrVixQna7Xf/6179UUFDgtpzl2LFjuuyyy6p1kAAAAMCvWaVnyh944AE5nU794Q9/0OrVqzV8+HDZ7XZJhZ/q9Mknn6hr167VPlAAAADg16rSM+V2u13r1q3Trl27FBwcrO7du7u2ORwOTZgwQT179qzWQQIAAAC/ZpUO5VLhJzrFxsZ6lIeEhGjChAkXPCgAAADgUlKlUC5J33//vTZt2qSUlBRJUsuWLdWvXz+1atWq2gYHAAAAXAoqvaZckl566SXdeOONmjt3rpYvX67ly5dr7ty5Gjx4sP72t79V9xgBAABQTR577GH1799X2dlZ5e4zffr9uvHGWOXm5p63vXXr1qhv32t04sSPrrLRo2/W7NlPVKmurz755GOtXLnco3zXrp3q2/ca7dq1s9JtmqnSM+WJiYlauHChunTpokmTJikyMlKSdOjQIS1evFgLFy5URESERo0aVe2DBQAAwIUZOvQW/d//JWvz5k+8fvT8yZMntGvXTo0YMVqBgYFV6uPZZ59Xw4ZBFzrUCm3a9E8dOnRQt912h1t5VNTVWrhwidq0aVOj/Ve3Sofy5cuXq1OnTnrrrbfk719SvVWrVoqJidHYsWO1bNkyQjkAAEAd1KtXH4WHh2vdug+9hvL169fKMAwNHTqsyn1ERl59IUO8IA0bBqlDh2jT+q+qSofyI0eO6KGHHnIL5K7G/P01ZMgQzZs3r1oGBwAAgOrl7++vQYOGaPnyt/T998fUqtUVrm2GYWjDho9kt0eqYcOGmj37Ce3Z85V++eUXhYaG6re/ba8pU36viIiWFfYxevTN6tKlm2bOfMJV9s03Xys+/iUdPLhfwcHBGjRoiFq08Gznk08+1tq1H+jo0SPKzs5S8+Yt1L//QN1xx3jXzH1c3GTt3r1LktS37zWSpGbNmisxcY127dqpBx6YogULFqpr12tc7SYlJWrVqpU6fjxFDRo01DXX9NCUKXFq3vw3rn3i4iYrKytLM2Y8pr//fb4OHjygxo2b6JZbRmjs2PGyWqu08tsnlQ7lAQEBOnPmTLnbs7OzFRAQcEGDAgAA+LXacXKXPjyyQWk56QqzheqWtoPVo1ntfsbLTTcN0/Llb2n9+rW6775prvLdu3fphx+Oa/r0Gfrll1MKCwvTtGl/UEhIiE6fPq2kpERNnnyXli17T2FhjX3u7+jRw5o+/X61aBGhmTOfkM1m06pVK/XJJ//02PeHH46rT5/rNGbMWNlsNh05clhLly5WSsoxzZr1tCTp4Ycf1YsvzlFKyjHNnv2CJCkwsPz8uXjxIi1Z8pqGDLlZ06b9QadP/6JFixI0ZcpEvfHGcrf38ssvP+uZZ/6i3/3uTk2ceJ+Sk7do0aJ4NWnSRDfeeJPP77myKh3Ko6Oj9e677+rWW29VkyZN3LalpqZq5cqV6tSpU7UNEAAA4Ndix8ldWr5/lfKceZKktJx0Ld+/SpJqNZi3atVaHTp01Mcfr9O9997vmgFev36tAgICNHDgYIWEhKpz55IxFRQU6Npr++rmmwdo48aPddttv/O5vzfeWCyr1aq//W2hwsLCJEm9e/fVnXfe6rHvhAmTXF8bhqGOHTsrODhYzz77pKZPn6FGjULUps2VCg4OVkBA4HmXqjgcDi1b9qauvz5W//u/f5Ek+ftbddVVUZo48U69++5yTZkS59o/IyNDL74Yr6iowiU43bv31O7du7Rx44a6FcqnTp2qu+66S0OGDNGoUaNcn+Z5+PBhrV69WtnZ2XrhhReqfaAAAAB1wfYTX+rzE/+uUt3/ZnyvfCPfrSzPmadl+xK17ccdlWqrd/Pu6tm8W5XGIRXe8Dl37jP697+3q2fP3jp79qy2bNmkvn1jFBISqry8PL333jtav36tTp48obNnz7rqfv/9d5Xq66uvvtQ11/R0BXJJ8vPzU//+g7RkyWtu+x4/nqI33nhdu3btVGrqLyooKHBtS0lJUfv2IZXq+9tvv1Zubo4GDhziVn7VVVG68kq7x1Namja9zBXIi7Vta9ehQwcq1W9lVTqUd+/eXS+//LKefvppLVmyxG3bb37zG82dO1fXXHNNObUBAAAuXWUD+fnKa1K/fgO0YMGLWrdujXr27K0tWz7R2bNnNHToLZKkBQvm6cMPV+vOO+9S585dFBQULIvFohkzpisnJ6dSfTkcGQoPD/coL1uWnZ2ladPuUf36DTRx4mS1bNlKNptNe/d+q3nz5ion51yl36fD4ZAkNW7srf8m+vHH425ljRp5hv7AwECfHg95Iar04UGxsbG6/vrr9c033+j48cI30rJlS7Vv314rV67UkCFDtG7duvO2k52drfnz52vDhg1yOByy2+2aNm2a+vXrV2G99957T6tWrdJ3332nrKwshYeHq1u3bpo6dapr5h4AAKAm9Gzercoz1I9/9qzSctI9ysNsofpD1ykXOrRKadCgoa6/vp82bdqozMxMrVu3Rpdddrl69OglSdq4cYMGDRqie++931UnLy9PmZmOSvfVqFGIUlNTPcrLlhXOjqcqPv6vbktnDh8+WOk+S/ctSadPe+v/F68h3AxVvoXUarWqY8eOGjJkiIYMGaLo6GhZrValpaXpv//9r09txMXFac2aNZo+fboWLVoku92uuLg4JScnV1gvLS1N1157rZ555hn94x//0PTp07Vv3z7deuutOnbsWFXfEgAAQI26pe1gBVjdb0gMsAbolraDTRnP0KG3KDc3R2+99Q/t2fOVBg8e6lpfbrFYPB7e8dFHH7gtJ/FV167dtHPndqWlpbnKCgoK9MknH7vtZ7FYJEn+/iX9GoahtWs/9GgzICDQpxn7Dh06KjDQpn/+033C+PDhQzp69LC6deteqfdSU6o0U14dkpOTtW3bNsXHx2vAgAGSpF69eiklJUVz5sxRTExMuXUnT57s9rpHjx7q1KmThgwZojVr1iguLq6cmgAAAOYpvpnT7KevFOvcuasiIlrpnXfeliTX0hVJuvbaPlq/fq2uuKK1rrzSrq+/3q0PPlitoKDgSvczYcIkbd36L02fPkUTJkySzVZPq1a96xGqO3TopKCgYL3wwl81adJkWSwWJSWtUnp6mkebV17ZVps3b9QHH6xWZGSUAgNtatvWc8VEcHCwxo+/W6+/vlDPPvukYmMHKC3tFy1a9IqaNGnq8eFDZjEtlG/cuFHBwcFuS1UsFotGjBihWbNm6fDhw5VailJ84wCPYwQAAHVZj2ZdTQvh3gwderMWLfq7OnfuqhYtIlzl06c/IqvVT2+++Q/l5OSofftozZsXrz/96cFK93HllXa99FKC4uNf0uzZT7ieU37DDf313HOzXfuFhoZq7tz5+vvfX9ITT8xUUFCQ+vcfpFGjxuiRR6a7tTlq1BgdOnRAr7yyQFlZWa7nlHtz1133KDQ0TKtWvauNGzeofv0G6t69p+6//wG3m0/NZDEMw6jOBl955RUtWLBA+/btq3C/MWPGyGKxaMWKFW7le/bs0W233ab58+dryJAh5dQuVFBQoIKCAh0/flwvvPCC9uzZo9WrV+vyyy+v9LhTU7PkdFbroTivpk2DdepUZq32CVyMOFcA33CuVL+TJ4+pWbMrzr8jLir+/lbl5zurvd3z/X2xWi0KDw/yPqZqH42P0tPT1bp1a4/ykJAQ1/bzufbaa137tW7dWm+++WaVAjkAAABgJp9CedlHH1Zk165dPu9bvJi/stuKLV26VOfOnVNKSoqWLl2q8ePH64033tBVV13l8xiKlXfVUtOaNq38uizgUsS5AviGc6V6/fyzVf7+NffR6jBPTXxfrVZrlc9Bn0L53LlzK9WoL4E6NDTU62x4RkaGpJIZ84pcfXXhg907d+6s2NhYDRo0SPPmzdMrr7xSqfFKLF8B6jLOFcA3nCvVz+l01sgyB5irppavOJ3OCs/BC16+8uabb1ZtZBWw2+365z//KafT6Xr0jiQdPFj4HMrIyMhKtdewYUO1bdtW3333XXUOEwAAAKhxPoXyHj16VHvHAwYMUGJiojZv3qz+/fu7ypOSktSmTZtKfwhQenq69u/fry5dulT3UAEAAIAaZdqNnjExMerZs6dmzpyp9PR0RUREKCkpSV9++aUSEhJc+40bN047duzQgQMHXGXDhg3TsGHD1KZNG9WvX1/fffed3nrrLZ07d05Tp0414+0AAAAAVWZaKLdYLEpISNC8efM0f/58ORwO2e12xcfHKzY2tsK6nTp10urVq/Xjjz8qJydH4eHh6t69u+bPn1/pZS8AAAAVMQzDp/vlcGm70KeMV/tzyi9W3OgJ1F2cK4BvOFeq36lTPygkpIkCA21mDwXVqCZu9MzNzVFGxi9q2rRFuftUdKMnz/gBAAAoR1BQqNLTTyk3N+eCZ0Lx62QYhnJzc5SefkpBQaFVbse05SsAAAB1Xf36DSVJGRm/qKAg3+TRoLpYrVY5ndU3U+7n56/g4DDX35eqIJQDAABUoH79hhcUtlD31MWlXixfAQAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATEYoBwAAAExGKAcAAABMRigHAAAATOZvZufZ2dmaP3++NmzYIIfDIbvdrmnTpqlfv34V1nvvvfe0adMmHThwQKmpqWrWrJmuu+46TZ06VY0bN66l0QMAAADVw9SZ8ri4OK1Zs0bTp0/XokWLZLfbFRcXp+Tk5ArrLViwQEFBQXrooYf0+uuv66677tL69es1evRoORyOWho9AAAAUD1MmylPTk7Wtm3bFB8frwEDBkiSevXqpZSUFM2ZM0cxMTHl1k1KSlJ4eLjrdY8ePWS32zVu3Dh98MEHGjduXI2PHwAAAKgups2Ub9y4UcHBwW5LVSwWi0aMGKGjR4/q8OHD5dYtHciLRUdHS5JOnjxZ/YMFAAAAapBpofzQoUOy2+2yWt2HEBUVJUk6ePBgpdr74osvJElXXXVV9QwQAAAAqCWmhfL09HSFhIR4lBeXpaenV6qtZ555Rq1bt9aQIUOqbYwAAABAbTD16SsWi6VK20o7e/aspk2bpoyMDL399tsKDAys0ljCw4OqVO9CNW0abEq/wMWGcwXwDecK4Ju6dq6YFspDQ0O9zoZnZGRIktdZ9LLOnTun+++/X3v37tXixYt19dVXV3k8qalZcjqNKteviqZNg3XqVGat9glcjDhXAN9wrgC+MetcsVot5U4Em7Z8xW6368iRI3I6nW7lxWvJIyMjK6yfk5OjqVOnavfu3Vq0aJG6du1aY2MFAAAAapJpoXzAgAFyOBzavHmzW3lSUpLatGkju91ebt3c3FxNnTpVO3fuVEJCgnr06FHTwwUAAABqjGnLV2JiYtSzZ0/NnDlT6enpioiIUFJSkr788kslJCS49hs3bpx27NihAwcOuMoeeOABbd26VdOmTVODBg20e/du17bGjRurVatWtfpeAAAAgAthMQyjdhdSl5KVlaV58+bp448/lsPhkN1u17Rp09S/f3/XPt5CefFjE70ZMWKE5syZU+mxsKYcqLs4VwDfcK4AvqmLa8pNDeV1CaEcqLs4VwDfcK4AvqmLody0NeUAAAAAChHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAkxHKAQAAAJMRygEAAACTEcoBAAAAk/mbPQAAAHBhPv/2pFYnH9FpR44aN7JpZExb9W7fzOxhAagEQjmAOouggYuVYRgyJMmQDBkyDMkoKjAMeWwrrFP2dek2Cncwivcr2skwpC8P/qxVyUeVl++UJKU6cvTG+v3KOpOnblFNZbFYJElWiySLRUX/c5VbLJJFFhW9lLWwoGi/kvLiOsXlwMWoLv9csRjFZ/YlLjU1S05n7R6Kpk2DdepUZq32CVwstn1zQks3HHAFDUny97NoYPeWuvqKMFdQqSjolN5WtMktzHgLTq5tRR0U1ykvHEmS0zDK3VbcjrN0v6X2K91HYVtlxlxmLJ5tuW8zVKp/t7Gc5xh5GYvnMXLfplLHxnMsZY6DYXi899JjcTvWRnnb3N+HUcExdbqN08tYvB4jL23KKPyeVDAWj78DlwhL0X+KA72lqKD4a0tRsrcWl8vLxUBRyC/cz/tFgvt+lqL2Si4YimoVXXR46bfseIra8Xk8pfry6LfseOTDcSh1weN28VPRcfBynL32W+Z9lb6AKrff84zHUua9eI6naN/ztec6ZqXGVu6xLarrpX9v3yuLl/ddth9ZpK8O/aLET4+4/VwJ9Ldqwo1X11owt1otCg8P8rqNUF6EUA7UHqdhKPNMntIzc3Q685zSMnPc/pzOzNFPp8+YPUxTlP1BKJX/Q94tALi+9gwNJW2W/IAuHRCK+yjdrrcf7q5tKucHrMXLtnKCkEqNxfOHeKlxld2v7Fi8tGkpb5xljlvpoOd2jNz2q+gYuQcGr98Hr8fIM/S4viflfL/KCzqS9Mb6/SrPXTde7XbBUHzRI3leTBb/CCy5YCt1oVjOBY9R3oVJ2f1K15e3fksuEksuZItHc572vJV77Ffq/Xm7QPTWb5nxuI7fecZT9oL3vP16uSD32u8lfAFY08Ib2fT81D610ldFoZzlKwCqVYHTqYysXJ3OzCkK3TlK8xK8C8pcBPtZLQoNClRosE0tLwuqMJQ/dmfXcsOR5D7TVPh/zyBXdpbJI+QVbysnQJYOT9ay27yEXbfgVk7IY0kAqmLNZ/9VqiPHozy8kU3XdfqNCSNCbTnvxYCX8tIXA95+Y+TtN2Wu30yVuUgor7y4p9IXGIbHeHzpp8x79DZOt/EU1TU8+5chLSnnAtbb+WMGQjkAn+XmFSgtq3TYLv3nnE5n5siRnev6R7lYgL9VYcE2NQ626aqIEIUG29Q4uJ7Cgm2uP40aBMpqLQmljyR8Vm7QuCoitKbfKnDRGBnTVkvX71dumV/Jj4xpa+KoUBtKTyoUTwmgfB9WcAFbFxDKAUiSzubklzurXfwn62yeR736Nn81DrYpNNimFk2DXF83DrYprCh4N6znX+lZYIIG4JvitbB19eY1oK6o6z9XWFNehDXl+LUyDEOZZ/PKzG6f81jDnZNb4FE3uEFA0Qx3YbguCduFf0KDbKpvq7lr+7p8lzxQF/FzBaiY2T9XuNHTB4RyXIycTkMZ2bmFN0s6cpSWVWpm23Gu6HWu8gucbvUsFik0yOaa1S4O3qHBgSUBPMimAP+68flinCuAbzhXAN+Yda5woydwEcrLd5Zav100s10meGdk5bqeAFDM38+qsOBAhQXXU9vfhJSZ4S4M3I0aBsjPWjcCNwAAMDmUZ2dna/78+dqwYYMcDofsdrumTZumfv36VVhv586dWrVqlfbu3avDhw8rPz9fBw4cqKVRAxfuXG6+a9mIW9B2zW7nKPOM5/ptW6CfGhcF7OatwxQWXK/MGm6bguoH8BQPAAAuMqaG8ri4OO3du1czZsxQRESE3n//fcXFxWnhwoWKiYkpt94XX3yhHTt2qH379vL399c333xTi6MGymcYhrLP5eu045zSs7yE7qL13GdzPNdvB9UPcK3VvrJ5o1Iz3CVPKanJ9dsAAMA8pq0pT05O1uTJkxUfH68BAwZIKgw0d9xxh9LT07V+/fpy6zqdTlmLfvU+e/Zsvfnmmxc8U86acpyP02nIcSa3cIbbkVMUuj2XlZT+pDCpcP12SMNAr7PapW+YDAzwM+md1X2cK4BvOFcA37CmvJSNGzcqODjYbamKxWLRiBEjNGvWLB0+fFh2u91rXStrYVHN8gucSs8sCdanHUWz2lklTypJz/Rcv+1ntbiCdetmwep6VVOP0B0SFMj6bQAAUCHTQvmhQ4dkt9s9AnZUVJQk6eDBg+WGcqAycnILP/AmzVH44TZuy0qKgrcjO9ejni3AzxWs27UK87hZMizYpqAGAa5PcwQAAKgq00J5enq6Wrdu7VEeEhLi2g5UxDAMncnJd1s6ctpxrtQMd2HwPpOT71G3YT1/V7i+olmwl2Ul9VTf5scNkwAAoFaYetdYRYGntsNQeet7alrTpsGm9FvXFT5/O0ep6ef0S8ZZpaafVarjnH5JP6vUjKL/O855fOBN8fO3w0Prq+XlweoSeZkah9RTk9D6ahJSX+Eh9dQ4pJ7qBXLD5MWGcwXwDecK4Ju6dq6YlkxCQ0O9zoZnZGRIKpkxry3c6Fl78gucysjKLZnR9rKsJD0rRwVOz/XboUE2hTWyqUWThurQpnGpGe56rvXb/n4Vrd82lJlxVpfeUb+4XarnClBZnCuAb7jRsxS73a5//vOfbk9SkQrXkktSZGSkWUPDBcjNK16/Xfzx7eeUnplb8pSSrBw5snJV9vIn0N/qWqcd2TJUjRvZXJ84GdbIprAgm4IbBrJ+GwAA/CqZFsoHDBigxMREbd68Wf3793eVJyUlqU2bNtzkWccYhqGzOQWuJ5EU/3HNcDsKn1KSfc5z/XYDm39hsA62qeVlQYUf6d6onlvobmDzZ/02AAC4ZJkWymNiYtSzZ0/NnDlT6enpioiIUFJSkr788kslJCS49hs3bpx27Njh9hzy06dPa8eOHZKk77//XpK0YcMGSVKLFi0UHR1di+/kWBC6TwAAC39JREFU4uc0DGWdyXP7cJvi2e7TmSVPKCm7fluSGjUMVFiQTU1C6umqliGFy0lcYbuewoJssgXy/G0AAICKmBbKLRaLEhISNG/ePM2fP18Oh0N2u13x8fGKjY2tsO6hQ4c0ffp0t7Li1yNGjNCcOXNqbNwXmwJnqfXbbjPc55ReaqY7v8B9QYnVYlFocKDCgm2KuCxI0VeGF81wl4Tu0GDbedZvAwAAwBemfaJnXVObN3p+/u1JrU4+otOOHDVuZNPImLbq3b5ZpdvJyy9QWlau0hznvITuwhnvjOxclf0OBxSv3y66aTKs6EbJ0KDC0B0WbFOjBoGyWllOgrqBm9cA33CuAL7hRk/o829Paun6/cot+ij2VEeOlq7fL0luwfxsTn6ZoH2uZA130f+zzuZ5tF/f5ud6/naLpuGu4O1aVtKonhrWY/02AABAXUIor2Wrk4+4Anmx3Hyn3tywX9v+c8K1nORsjuf67eAGAa4nlFzZIqRohrvkQ29Cg2yqb+NbCgAAcLEhwdWyVEeO1/KcPKfO5hboN00aqn2bxq7w3Ti4nkKDbQoLClSAPzdMAgAA/BoRymtZeCOb12Ae3simx8dfY8KIAAAAYDYenVHLRsa0VaC/+2EP9LdqZExbk0YEAAAAszFTXsuKb+asjqevAAAA4NeBUG6C3u2bqXf7Zjy6CgAAAJJYvgIAAACYjlAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMUA4AAACYjFAOAAAAmIxQDgAAAJiMT/QsYrVaLql+gYsN5wrgG84VwDdmnCsV9WkxDMOoxbEAAAAAKIPlKwAAAIDJCOUAAACAyQjlAAAAgMkI5QAAAMD/t3f/IVHfDxzHX5pTWuYqsz+SS6JtiXqb/SGpKzUtGNFwjMCVnW0zt9AkaxOC5qBlC7KoPFs1JTYZFOvHkC6otITabpuDmric2Ji1RLTwRycYG3a3P6Lb186+a3+090d9PkDi87733b1OkF6+/Xw+b8Mo5QAAAIBhlHIAAADAMEo5AAAAYBilHAAAADCMUg4AAAAYFmI6wETT3d2tmpoaXbt2TW1tbRoaGlJtba0WLlxoOhpgGd99953q6up09epVdXd367nnntNLL72k4uJizZ8/33Q8wDKuXLmiAwcOqL29XQMDA5oyZYpefPFF5efnKz093XQ8wNKcTqeqqqoUGxururo603FYKf+v3bx5U2fOnNGzzz6r5ORk03EASzp69Ki6urr01ltvqbq6Wlu2bFFXV5dWrlypn376yXQ8wDI8Ho/mzp2rLVu2qKamRtu3b1doaKjeffddnTlzxnQ8wLKuX7+u6upqzZw503QUvyCfz+czHWIi8Xq9Cg5+8LtQQ0ODioqKWCkHHtHb26vIyMgRYx6PR1lZWUpOTpbT6TSUDLC+4eFhZWVlKSYmRrW1tabjAJbj9Xr15ptvym63q729XR6Ph5XyiehhIQfweI8WckmKiIhQTEyMuru7DSQCxo6QkBBNnTpVzzzzjOkogCV9/vnn6u7u1qZNm0xHGYGGCGBM6Ovr0/Xr1/XCCy+YjgJYjtfr1fDwsHp6elRZWakbN25o7dq1pmMBlnPr1i1VVlbqo48+Unh4uOk4I3ChJwDL8/l8Kisrk9frVX5+vuk4gOWUlJTo3LlzkqTw8HDt27dPaWlphlMB1uLz+fThhx9q0aJFWrp0qek4AVgpB2B5u3btUkNDg7Zt26Z58+aZjgNYTmlpqY4fP66DBw8qPT1dJSUlcrlcpmMBlvLVV1/p559/VllZmekoo2KlHICl7d27V0eOHNHWrVv1xhtvmI4DWJLNZpPNZpMkZWZmav369fr444+1fPlyrmUC9OAUyIqKCr333nuaPHmyPB6PpAcXRnu9Xnk8HoWFhSksLMxYRn5SAVjW/v37dejQIZWWliovL890HGDMsNvtunv3rvr6+kxHASyhp6dHg4OD2rNnj5KSkvxfV65cUXt7u5KSkozf2YuVcgCWVFVVpU8//VQbN27UunXrTMcBxgyfz6empiZFRERo2rRppuMAljBnzpxRbxH6ySefaGhoSOXl5Zo9e7aBZH+jlBtw9uxZSVJLS4sk6ccff1R/f78mT57MDmyApCNHjsjpdGrJkiVKTU0dsWFQaGio4uLiDKYDrOP9999XdHS04uPjNX36dN25c0dff/21vv/+e5WVlSkkhP/mAUmaMmXKqHvCRERESJIl9oth8yADHrdNeHR0tC5evPgfpwGsx+FwqKmpadTH+DkB/vbll1/q9OnTunHjhgYHBzV16lQlJCQoNzdXmZmZpuMBludwOCyzeRClHAAAADCMCz0BAAAAwyjlAAAAgGGUcgAAAMAwSjkAAABgGKUcAAAAMIxSDgAAABhGKQcAGONwOLifNgCIHT0BYNz54YcflJeX99jHJ02apNbW1v8wEQDgn1DKAWCcWrFihdLS0gLGg4P5IykAWA2lHADGqbi4OGVnZ5uOAQB4AiyXAMAE1dnZqfnz58vpdMrlcum1116T3W5XRkaGnE6nhoeHA57T1tamoqIiLVy4UHa7XcuXL1d1dbXu378fMPfOnTsqLy9XVlaWEhISlJKSorffflvffvttwNyenh5t3rxZSUlJSkxMVH5+vjo6Op7K5wYAK2KlHADGqXv37qmvry9gPDQ0VOHh4f7jxsZGffHFF8rNzdXMmTN18eJFVVVVqaurSzt37vTPa2lpkcPhUEhIiH9uY2Ojdu/erba2Nu3Zs8c/t7OzU6tWrVJvb6+ys7OVkJCge/fuqbm5WW63W6+88op/7tDQkNasWaOXX35ZmzZtUmdnp2pra1VYWCiXy6VJkyY9pe8QAFgHpRwAximn0ymn0xkwnpGRocOHD/uPf/nlF504cULx8fGSpDVr1mjDhg06deqUcnJylJiYKEnasWOH/vzzTx07dkyxsbH+uSUlJXK5XFq5cqVSUlIkSdu2bdPt27dVU1OjxYsXj3h/r9c74ri/v1/5+fkqKCjwj82YMUMVFRVyu90BzweA8YhSDgDjVE5Ojl599dWA8RkzZow4Tk1N9RdySQoKCtK6devU0NCg+vp6JSYmqre3V1evXtWyZcv8hfzh3PXr1+vs2bOqr69XSkqKBgYGdPnyZS1evHjUQv3ohabBwcEBd4tJTk6WJN28eZNSDmBCoJQDwDgVExOj1NTUf5w3b968gLHnn39eknTr1i1JD05H+d/xR58fHBzsn/v777/L5/MpLi7uiXLOmjVLYWFhI8amTZsmSRoYGHii1wCAsY4LPQFgggsKCvrHOT6f74lf7+HcJ3ldSf/3nPF/874AMJZRygFggvv1118fO2az2Ub8O9rc3377TV6v1z8nJiZGQUFBbFAEAP8CpRwAJji3261r1675j30+n2pqaiRJS5culSRFRkZqwYIFamxsVHt7+4i5n332mSRp2bJlkh6cepKWlqZLly7J7XYHvB+r3wAQiHPKAWCcam1tVV1d3aiPPSzbkhQbG6u1a9cqNzdXUVFRunDhgtxut7Kzs7VgwQL/vK1bt8rhcCg3N1erV69WVFSUGhsb9c0332jFihX+O69IUllZmVpbW1VQUKDXX39d8fHx+uOPP9Tc3Kzo6GiVlpY+vQ8OAGMQpRwAximXyyWXyzXqY+fPn/efy52Zmam5c+fq8OHD6ujoUGRkpAoLC1VYWDjiOXa7XceOHVNlZaWOHj2qoaEh2Ww2ffDBB3rnnXdGzLXZbDp58qQOHDigS5cuqa6uThEREYqNjVVOTs7T+cAAMIYF+fg7IgBMSJ2dncrKytKGDRtUXFxsOg4ATGicUw4AAAAYRikHAAAADKOUAwAAAIZxTjkAAABgGCvlAAAAgGGUcgAAAMAwSjkAAABgGKUcAAAAMIxSDgAAABhGKQcAAAAM+wtRq62zftUBTAAAAABJRU5ErkJggg=="}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"43ff1aae-562b-4673-8523-23ff6d753c6f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"BERT Model Tuning","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1194043512476967}},"nbformat":4,"nbformat_minor":0}
